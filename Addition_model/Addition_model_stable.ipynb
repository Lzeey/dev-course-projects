{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.0.8\n",
      "Tensorflow version: 1.3.0\n",
      "Numpy version: 1.13.1\n",
      "Pandas version: 0.18.1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Keras version: {}\".format(keras.__version__))\n",
    "print(\"Tensorflow version: {}\".format(tf.__version__))\n",
    "print(\"Numpy version: {}\".format(np.__version__))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network to sum\n",
    "\n",
    "In this particular exercise, we train a fully connected network to do a simple addition.\n",
    "\n",
    "Input: 2 integers, from 0-100\n",
    "\n",
    "Twist: train this as a classification problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48  6]\n",
      " [98 91]\n",
      " [ 6 84]\n",
      " [25 74]\n",
      " [30 25]] [ 54 189  90  99  55]\n"
     ]
    }
   ],
   "source": [
    "# Generate samples\n",
    "num_samples = 50000\n",
    "input_range_lo = 0 # inclusive\n",
    "input_range_hi = 100\n",
    "n_classes = (input_range_hi * 2) - 2 * input_range_lo + 1\n",
    "\n",
    "def generate_samples(num_samples, lo, hi):\n",
    "    X = pd.DataFrame(np.random.randint(lo, hi+1, size=2*num_samples,).reshape(num_samples, 2))\n",
    "    \n",
    "    X['y'] = X[0] + X[1]\n",
    "    \n",
    "    #Ensure uniqueness\n",
    "    #X.drop_duplicates(inplace=True) ##Makes problem much harder\n",
    "    \n",
    "    #TODO: Drop some numbers\n",
    "    return X[[0, 1]].values, X['y'].values\n",
    "\n",
    "X, y = generate_samples(num_samples, input_range_lo, input_range_hi)\n",
    "print(X[:5], y[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be truely data-agnostic, we should code the normalisation and min-max finding process to fit the generates samples.\n",
    "\n",
    "But I'm lazy, so lets use our prior knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 2) (40000, 201)\n",
      "(10000, 2) (10000, 201)\n",
      "[[33 27]\n",
      " [54 45]\n",
      " [ 9 36]\n",
      " [36 31]\n",
      " [22 24]]\n"
     ]
    }
   ],
   "source": [
    "#Normalize data to 0->1, and perform OHE\n",
    "def transform_X(X):\n",
    "    X_float = X.astype(np.float32) - input_range_lo\n",
    "    X_float /= (input_range_hi - input_range_lo)\n",
    "    return X_float\n",
    "\n",
    "def invert_X(X_float):\n",
    "    X = X_float * (input_range_hi - input_range_lo)\n",
    "    X += input_range_lo\n",
    "    X = X.astype(int)\n",
    "    return X\n",
    "\n",
    "one_hot_labels = keras.utils.to_categorical(y, num_classes=n_classes)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, one_hot_labels, test_size=0.2)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "print(X_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick a few random numbers to remove, and add these subsequently to the test set later\n",
    "\n",
    "**Note: Running this cell multiple times will reduce the number of samples again!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing [23 71 99]\n",
      "Removed 2317 samples. Now training with  37683\n"
     ]
    }
   ],
   "source": [
    "num_to_hide = np.random.choice(range(input_range_lo, input_range_hi+1), size=3, replace=False)\n",
    "print(\"Removing\", num_to_hide)\n",
    "\n",
    "def remove_numbers(X, y, num_to_hide):\n",
    "    old_len = len(X)\n",
    "    for num in num_to_hide:\n",
    "        y = y[(X[:,0]!=num) & (X[:,1]!=num)]\n",
    "        X = X[(X[:,0]!=num) & (X[:,1]!=num) ,:]\n",
    "    print(\"Removed\", old_len - len(X), \"samples. Now training with \", len(X), \"samples\")\n",
    "    assert(len(X) == len(y))\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = remove_numbers(X_train, y_train, num_to_hide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Normalize data\n",
    "X_train = transform_X(X_train)\n",
    "X_test = transform_X(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "Dense_0 (Dense)              (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "Dense_1 (Dense)              (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "Outputlayer (Dense)          (None, 201)               25929     \n",
      "=================================================================\n",
      "Total params: 34,441\n",
      "Trainable params: 34,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Generate model here\n",
    "\n",
    "def gen_model(n_classes, n_hidden_list=[64,128], activation='elu'):\n",
    "    inputs = Input(shape=(2,))\n",
    "    x = Dense(n_hidden_list[0], activation=activation, name = \"Dense_0\")(inputs)\n",
    "    if len(n_hidden_list) > 1:\n",
    "        for idx, n_hidden in enumerate(n_hidden_list[1:]):\n",
    "            x = Dense(n_hidden, activation=activation, name = \"Dense_\"+str(idx+1))(x)\n",
    "    outputs = Dense(n_classes, activation='softmax', name = \"Outputlayer\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "                  optimizer='adamax',\n",
    "                 metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "model = gen_model(n_classes)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32030 samples, validate on 5653 samples\n",
      "Epoch 1/200\n",
      "31232/32030 [============================>.] - ETA: 0s - loss: 4.5797 - acc: 0.0303Epoch 00000: val_acc improved from -inf to 0.07748, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 4.5632 - acc: 0.0313 - val_loss: 3.8743 - val_acc: 0.0775\n",
      "Epoch 2/200\n",
      "31136/32030 [============================>.] - ETA: 0s - loss: 3.5320 - acc: 0.0862Epoch 00001: val_acc improved from 0.07748 to 0.11233, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 3.5233 - acc: 0.0868 - val_loss: 3.2148 - val_acc: 0.1123\n",
      "Epoch 3/200\n",
      "31008/32030 [============================>.] - ETA: 0s - loss: 3.0397 - acc: 0.1335Epoch 00002: val_acc improved from 0.11233 to 0.14559, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 3.0340 - acc: 0.1342 - val_loss: 2.8669 - val_acc: 0.1456\n",
      "Epoch 4/200\n",
      "31456/32030 [============================>.] - ETA: 0s - loss: 2.7452 - acc: 0.1664Epoch 00003: val_acc improved from 0.14559 to 0.18167, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 2.7430 - acc: 0.1665 - val_loss: 2.6218 - val_acc: 0.1817\n",
      "Epoch 5/200\n",
      "31872/32030 [============================>.] - ETA: 0s - loss: 2.5396 - acc: 0.2020Epoch 00004: val_acc improved from 0.18167 to 0.22112, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 2.5391 - acc: 0.2022 - val_loss: 2.4686 - val_acc: 0.2211\n",
      "Epoch 6/200\n",
      "31616/32030 [============================>.] - ETA: 0s - loss: 2.3910 - acc: 0.2255Epoch 00005: val_acc improved from 0.22112 to 0.24589, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 2.3889 - acc: 0.2268 - val_loss: 2.3357 - val_acc: 0.2459\n",
      "Epoch 7/200\n",
      "31424/32030 [============================>.] - ETA: 0s - loss: 2.2697 - acc: 0.2505Epoch 00006: val_acc improved from 0.24589 to 0.25367, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 2.2692 - acc: 0.2505 - val_loss: 2.1920 - val_acc: 0.2537\n",
      "Epoch 8/200\n",
      "31744/32030 [============================>.] - ETA: 0s - loss: 2.1690 - acc: 0.2736Epoch 00007: val_acc improved from 0.25367 to 0.32160, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 2.1691 - acc: 0.2734 - val_loss: 2.1149 - val_acc: 0.3216\n",
      "Epoch 9/200\n",
      "31616/32030 [============================>.] - ETA: 0s - loss: 2.0868 - acc: 0.2926Epoch 00008: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 2.0863 - acc: 0.2928 - val_loss: 2.0117 - val_acc: 0.3018\n",
      "Epoch 10/200\n",
      "31840/32030 [============================>.] - ETA: 0s - loss: 2.0189 - acc: 0.3095Epoch 00009: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 2.0184 - acc: 0.3095 - val_loss: 1.9989 - val_acc: 0.3161\n",
      "Epoch 11/200\n",
      "31232/32030 [============================>.] - ETA: 0s - loss: 1.9541 - acc: 0.3256Epoch 00010: val_acc improved from 0.32160 to 0.36459, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.9537 - acc: 0.3251 - val_loss: 1.9234 - val_acc: 0.3646\n",
      "Epoch 12/200\n",
      "31744/32030 [============================>.] - ETA: 0s - loss: 1.8952 - acc: 0.3421Epoch 00011: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.8955 - acc: 0.3417 - val_loss: 1.8456 - val_acc: 0.3531\n",
      "Epoch 13/200\n",
      "31168/32030 [============================>.] - ETA: 0s - loss: 1.8479 - acc: 0.3564Epoch 00012: val_acc improved from 0.36459 to 0.37361, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.8477 - acc: 0.3567 - val_loss: 1.7945 - val_acc: 0.3736\n",
      "Epoch 14/200\n",
      "31936/32030 [============================>.] - ETA: 0s - loss: 1.7982 - acc: 0.3745Epoch 00013: val_acc improved from 0.37361 to 0.38281, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.7978 - acc: 0.3745 - val_loss: 1.7389 - val_acc: 0.3828\n",
      "Epoch 15/200\n",
      "31040/32030 [============================>.] - ETA: 0s - loss: 1.7580 - acc: 0.3875Epoch 00014: val_acc improved from 0.38281 to 0.39342, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.7564 - acc: 0.3878 - val_loss: 1.7448 - val_acc: 0.3934\n",
      "Epoch 16/200\n",
      "31520/32030 [============================>.] - ETA: 0s - loss: 1.7256 - acc: 0.3962Epoch 00015: val_acc improved from 0.39342 to 0.43446, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.7261 - acc: 0.3959 - val_loss: 1.6968 - val_acc: 0.4345\n",
      "Epoch 17/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 1.6801 - acc: 0.4084Epoch 00016: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 1.6794 - acc: 0.4086 - val_loss: 1.6272 - val_acc: 0.4215\n",
      "Epoch 18/200\n",
      "31840/32030 [============================>.] - ETA: 0s - loss: 1.6529 - acc: 0.4205Epoch 00017: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.6525 - acc: 0.4205 - val_loss: 1.6885 - val_acc: 0.3874\n",
      "Epoch 19/200\n",
      "31808/32030 [============================>.] - ETA: 0s - loss: 1.6208 - acc: 0.4312Epoch 00018: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.6209 - acc: 0.4316 - val_loss: 1.5897 - val_acc: 0.4343\n",
      "Epoch 20/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 1.5930 - acc: 0.4368Epoch 00019: val_acc improved from 0.43446 to 0.44065, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.5926 - acc: 0.4368 - val_loss: 1.5471 - val_acc: 0.4407\n",
      "Epoch 21/200\n",
      "31872/32030 [============================>.] - ETA: 0s - loss: 1.5589 - acc: 0.4541Epoch 00020: val_acc improved from 0.44065 to 0.44437, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.5593 - acc: 0.4538 - val_loss: 1.5316 - val_acc: 0.4444\n",
      "Epoch 22/200\n",
      "31520/32030 [============================>.] - ETA: 0s - loss: 1.5341 - acc: 0.4626Epoch 00021: val_acc improved from 0.44437 to 0.45869, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.5342 - acc: 0.4623 - val_loss: 1.5314 - val_acc: 0.4587\n",
      "Epoch 23/200\n",
      "31776/32030 [============================>.] - ETA: 0s - loss: 1.5059 - acc: 0.4742Epoch 00022: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.5055 - acc: 0.4742 - val_loss: 1.4475 - val_acc: 0.4484\n",
      "Epoch 24/200\n",
      "31904/32030 [============================>.] - ETA: 0s - loss: 1.4783 - acc: 0.4803Epoch 00023: val_acc improved from 0.45869 to 0.50221, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.4779 - acc: 0.4807 - val_loss: 1.4669 - val_acc: 0.5022\n",
      "Epoch 25/200\n",
      "31744/32030 [============================>.] - ETA: 0s - loss: 1.4583 - acc: 0.4884Epoch 00024: val_acc improved from 0.50221 to 0.52291, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.4590 - acc: 0.4879 - val_loss: 1.3981 - val_acc: 0.5229\n",
      "Epoch 26/200\n",
      "31488/32030 [============================>.] - ETA: 0s - loss: 1.4283 - acc: 0.5050Epoch 00025: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.4278 - acc: 0.5052 - val_loss: 1.4871 - val_acc: 0.4760\n",
      "Epoch 27/200\n",
      "31616/32030 [============================>.] - ETA: 0s - loss: 1.4129 - acc: 0.5072Epoch 00026: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.4148 - acc: 0.5066 - val_loss: 1.3681 - val_acc: 0.5089\n",
      "Epoch 28/200\n",
      "31904/32030 [============================>.] - ETA: 0s - loss: 1.3895 - acc: 0.5230Epoch 00027: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.3899 - acc: 0.5229 - val_loss: 1.3614 - val_acc: 0.4987\n",
      "Epoch 29/200\n",
      "31168/32030 [============================>.] - ETA: 0s - loss: 1.3760 - acc: 0.5277Epoch 00028: val_acc improved from 0.52291 to 0.54820, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.3754 - acc: 0.5275 - val_loss: 1.3065 - val_acc: 0.5482\n",
      "Epoch 30/200\n",
      "31456/32030 [============================>.] - ETA: 0s - loss: 1.3520 - acc: 0.5376Epoch 00029: val_acc improved from 0.54820 to 0.55935, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.3517 - acc: 0.5378 - val_loss: 1.3357 - val_acc: 0.5593\n",
      "Epoch 31/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 1.3360 - acc: 0.5456Epoch 00030: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.3374 - acc: 0.5444 - val_loss: 1.2919 - val_acc: 0.5563\n",
      "Epoch 32/200\n",
      "31648/32030 [============================>.] - ETA: 0s - loss: 1.3098 - acc: 0.5537Epoch 00031: val_acc improved from 0.55935 to 0.59243, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 1.3095 - acc: 0.5535 - val_loss: 1.2310 - val_acc: 0.5924\n",
      "Epoch 33/200\n",
      "31296/32030 [============================>.] - ETA: 0s - loss: 1.2891 - acc: 0.5627Epoch 00032: val_acc improved from 0.59243 to 0.65275, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 1.2903 - acc: 0.5624 - val_loss: 1.2582 - val_acc: 0.6528\n",
      "Epoch 34/200\n",
      "31296/32030 [============================>.] - ETA: 0s - loss: 1.2736 - acc: 0.5758Epoch 00033: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 1.2752 - acc: 0.5757 - val_loss: 1.2458 - val_acc: 0.5597\n",
      "Epoch 35/200\n",
      "31168/32030 [============================>.] - ETA: 0s - loss: 1.2593 - acc: 0.5838Epoch 00034: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.2590 - acc: 0.5832 - val_loss: 1.2371 - val_acc: 0.5731\n",
      "Epoch 36/200\n",
      "31424/32030 [============================>.] - ETA: 0s - loss: 1.2421 - acc: 0.5889Epoch 00035: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.2423 - acc: 0.5889 - val_loss: 1.2146 - val_acc: 0.5815\n",
      "Epoch 37/200\n",
      "31744/32030 [============================>.] - ETA: 0s - loss: 1.2239 - acc: 0.5984Epoch 00036: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.2230 - acc: 0.5992 - val_loss: 1.2212 - val_acc: 0.5923\n",
      "Epoch 38/200\n",
      "31104/32030 [============================>.] - ETA: 0s - loss: 1.2193 - acc: 0.6011Epoch 00037: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.2194 - acc: 0.6012 - val_loss: 1.2400 - val_acc: 0.5549\n",
      "Epoch 39/200\n",
      "31072/32030 [============================>.] - ETA: 0s - loss: 1.1960 - acc: 0.6077Epoch 00038: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.1950 - acc: 0.6086 - val_loss: 1.1922 - val_acc: 0.6009\n",
      "Epoch 40/200\n",
      "31840/32030 [============================>.] - ETA: 0s - loss: 1.1795 - acc: 0.6189Epoch 00039: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.1799 - acc: 0.6187 - val_loss: 1.2300 - val_acc: 0.5799\n",
      "Epoch 41/200\n",
      "31552/32030 [============================>.] - ETA: 0s - loss: 1.1657 - acc: 0.6240Epoch 00040: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.1659 - acc: 0.6233 - val_loss: 1.1325 - val_acc: 0.6524\n",
      "Epoch 42/200\n",
      "31200/32030 [============================>.] - ETA: 0s - loss: 1.1520 - acc: 0.6337Epoch 00041: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.1521 - acc: 0.6331 - val_loss: 1.1738 - val_acc: 0.6149\n",
      "Epoch 43/200\n",
      "31776/32030 [============================>.] - ETA: 0s - loss: 1.1319 - acc: 0.6436Epoch 00042: val_acc improved from 0.65275 to 0.66460, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.1329 - acc: 0.6431 - val_loss: 1.1112 - val_acc: 0.6646\n",
      "Epoch 44/200\n",
      "31264/32030 [============================>.] - ETA: 0s - loss: 1.1222 - acc: 0.6452Epoch 00043: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.1217 - acc: 0.6458 - val_loss: 1.0608 - val_acc: 0.6499\n",
      "Epoch 45/200\n",
      "31264/32030 [============================>.] - ETA: 0s - loss: 1.1075 - acc: 0.6614Epoch 00044: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.1066 - acc: 0.6614 - val_loss: 1.0855 - val_acc: 0.6163\n",
      "Epoch 46/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 1.1047 - acc: 0.6588Epoch 00045: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.1056 - acc: 0.6593 - val_loss: 1.0505 - val_acc: 0.6223\n",
      "Epoch 47/200\n",
      "31616/32030 [============================>.] - ETA: 0s - loss: 1.0888 - acc: 0.6656Epoch 00046: val_acc improved from 0.66460 to 0.67929, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.0886 - acc: 0.6657 - val_loss: 1.0720 - val_acc: 0.6793\n",
      "Epoch 48/200\n",
      "31936/32030 [============================>.] - ETA: 0s - loss: 1.0705 - acc: 0.6749Epoch 00047: val_acc improved from 0.67929 to 0.71095, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.0704 - acc: 0.6752 - val_loss: 1.0477 - val_acc: 0.7109\n",
      "Epoch 49/200\n",
      "31552/32030 [============================>.] - ETA: 0s - loss: 1.0532 - acc: 0.6833Epoch 00048: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.0532 - acc: 0.6833 - val_loss: 1.0209 - val_acc: 0.6788\n",
      "Epoch 50/200\n",
      "31680/32030 [============================>.] - ETA: 0s - loss: 1.0563 - acc: 0.6850Epoch 00049: val_acc improved from 0.71095 to 0.71325, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.0567 - acc: 0.6850 - val_loss: 1.0116 - val_acc: 0.7132\n",
      "Epoch 51/200\n",
      "31456/32030 [============================>.] - ETA: 0s - loss: 1.0259 - acc: 0.7028Epoch 00050: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.0268 - acc: 0.7023 - val_loss: 1.0076 - val_acc: 0.6901\n",
      "Epoch 52/200\n",
      "31552/32030 [============================>.] - ETA: 0s - loss: 1.0334 - acc: 0.6963Epoch 00051: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 1.0326 - acc: 0.6968 - val_loss: 1.0244 - val_acc: 0.6455\n",
      "Epoch 53/200\n",
      "31104/32030 [============================>.] - ETA: 0s - loss: 1.0056 - acc: 0.7047Epoch 00052: val_acc improved from 0.71325 to 0.71626, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 1.0059 - acc: 0.7043 - val_loss: 0.9721 - val_acc: 0.7163\n",
      "Epoch 54/200\n",
      "31424/32030 [============================>.] - ETA: 0s - loss: 0.9949 - acc: 0.7116Epoch 00053: val_acc improved from 0.71626 to 0.74810, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.9960 - acc: 0.7116 - val_loss: 0.9570 - val_acc: 0.7481\n",
      "Epoch 55/200\n",
      "31008/32030 [============================>.] - ETA: 0s - loss: 0.9782 - acc: 0.7214Epoch 00054: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.9769 - acc: 0.7219 - val_loss: 0.9613 - val_acc: 0.6885\n",
      "Epoch 56/200\n",
      "31104/32030 [============================>.] - ETA: 0s - loss: 0.9831 - acc: 0.7138Epoch 00055: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.9828 - acc: 0.7144 - val_loss: 0.9654 - val_acc: 0.6924\n",
      "Epoch 57/200\n",
      "31616/32030 [============================>.] - ETA: 0s - loss: 0.9636 - acc: 0.7214Epoch 00056: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.9638 - acc: 0.7217 - val_loss: 0.9638 - val_acc: 0.7228\n",
      "Epoch 58/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 0.9574 - acc: 0.7259Epoch 00057: val_acc improved from 0.74810 to 0.77852, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.9569 - acc: 0.7264 - val_loss: 0.8902 - val_acc: 0.7785\n",
      "Epoch 59/200\n",
      "31136/32030 [============================>.] - ETA: 0s - loss: 0.9487 - acc: 0.7328Epoch 00058: val_acc improved from 0.77852 to 0.78100, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.9478 - acc: 0.7330 - val_loss: 0.9111 - val_acc: 0.7810\n",
      "Epoch 60/200\n",
      "31232/32030 [============================>.] - ETA: 0s - loss: 0.9348 - acc: 0.7331Epoch 00059: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.9359 - acc: 0.7331 - val_loss: 0.8956 - val_acc: 0.7219\n",
      "Epoch 61/200\n",
      "31104/32030 [============================>.] - ETA: 0s - loss: 0.9252 - acc: 0.7396Epoch 00060: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.9246 - acc: 0.7405 - val_loss: 0.9013 - val_acc: 0.7534\n",
      "Epoch 62/200\n",
      "31616/32030 [============================>.] - ETA: 0s - loss: 0.9166 - acc: 0.7432Epoch 00061: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.9168 - acc: 0.7433 - val_loss: 0.9263 - val_acc: 0.7217\n",
      "Epoch 63/200\n",
      "31328/32030 [============================>.] - ETA: 0s - loss: 0.9067 - acc: 0.7459Epoch 00062: val_acc improved from 0.78100 to 0.78348, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.9065 - acc: 0.7466 - val_loss: 0.8312 - val_acc: 0.7835\n",
      "Epoch 64/200\n",
      "31584/32030 [============================>.] - ETA: 0s - loss: 0.8880 - acc: 0.7536Epoch 00063: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.8884 - acc: 0.7531 - val_loss: 0.8816 - val_acc: 0.7295\n",
      "Epoch 65/200\n",
      "31872/32030 [============================>.] - ETA: 0s - loss: 0.8822 - acc: 0.7564Epoch 00064: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.8823 - acc: 0.7564 - val_loss: 0.8427 - val_acc: 0.7704\n",
      "Epoch 66/200\n",
      "31008/32030 [============================>.] - ETA: 0s - loss: 0.8630 - acc: 0.7608Epoch 00065: val_acc improved from 0.78348 to 0.78737, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.8627 - acc: 0.7604 - val_loss: 0.8204 - val_acc: 0.7874\n",
      "Epoch 67/200\n",
      "31424/32030 [============================>.] - ETA: 0s - loss: 0.8659 - acc: 0.7657Epoch 00066: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.8659 - acc: 0.7657 - val_loss: 0.8259 - val_acc: 0.7759\n",
      "Epoch 68/200\n",
      "32000/32030 [============================>.] - ETA: 0s - loss: 0.8451 - acc: 0.7712Epoch 00067: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.8450 - acc: 0.7712 - val_loss: 0.8471 - val_acc: 0.7679\n",
      "Epoch 69/200\n",
      "31168/32030 [============================>.] - ETA: 0s - loss: 0.8393 - acc: 0.7732Epoch 00068: val_acc improved from 0.78737 to 0.78843, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.8394 - acc: 0.7728 - val_loss: 0.8086 - val_acc: 0.7884\n",
      "Epoch 70/200\n",
      "31904/32030 [============================>.] - ETA: 0s - loss: 0.8413 - acc: 0.7708Epoch 00069: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.8412 - acc: 0.7709 - val_loss: 0.8572 - val_acc: 0.7582\n",
      "Epoch 71/200\n",
      "31968/32030 [============================>.] - ETA: 0s - loss: 0.8298 - acc: 0.7792Epoch 00070: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.8294 - acc: 0.7793 - val_loss: 0.8178 - val_acc: 0.7477\n",
      "Epoch 72/200\n",
      "31680/32030 [============================>.] - ETA: 0s - loss: 0.8134 - acc: 0.7844Epoch 00071: val_acc improved from 0.78843 to 0.79409, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 0.8126 - acc: 0.7846 - val_loss: 0.7786 - val_acc: 0.7941\n",
      "Epoch 73/200\n",
      "31744/32030 [============================>.] - ETA: 0s - loss: 0.8044 - acc: 0.7857Epoch 00072: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.8041 - acc: 0.7857 - val_loss: 0.7787 - val_acc: 0.7792\n",
      "Epoch 74/200\n",
      "31456/32030 [============================>.] - ETA: 0s - loss: 0.8050 - acc: 0.7830Epoch 00073: val_acc improved from 0.79409 to 0.82151, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.8046 - acc: 0.7836 - val_loss: 0.7451 - val_acc: 0.8215\n",
      "Epoch 75/200\n",
      "31392/32030 [============================>.] - ETA: 0s - loss: 0.7929 - acc: 0.7908Epoch 00074: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.7930 - acc: 0.7910 - val_loss: 0.7826 - val_acc: 0.7936\n",
      "Epoch 76/200\n",
      "31968/32030 [============================>.] - ETA: 0s - loss: 0.7759 - acc: 0.7960Epoch 00075: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.7760 - acc: 0.7960 - val_loss: 0.7491 - val_acc: 0.8120\n",
      "Epoch 77/200\n",
      "31808/32030 [============================>.] - ETA: 0s - loss: 0.7733 - acc: 0.7975Epoch 00076: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.7736 - acc: 0.7974 - val_loss: 0.7331 - val_acc: 0.8127\n",
      "Epoch 78/200\n",
      "31584/32030 [============================>.] - ETA: 0s - loss: 0.7649 - acc: 0.7984Epoch 00077: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.7639 - acc: 0.7987 - val_loss: 0.7476 - val_acc: 0.7941\n",
      "Epoch 79/200\n",
      "31424/32030 [============================>.] - ETA: 0s - loss: 0.7528 - acc: 0.8024Epoch 00078: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.7535 - acc: 0.8022 - val_loss: 0.7820 - val_acc: 0.7964\n",
      "Epoch 80/200\n",
      "31872/32030 [============================>.] - ETA: 0s - loss: 0.7511 - acc: 0.8002Epoch 00079: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.7499 - acc: 0.8006 - val_loss: 0.7977 - val_acc: 0.7990\n",
      "Epoch 81/200\n",
      "31136/32030 [============================>.] - ETA: 0s - loss: 0.7438 - acc: 0.8037Epoch 00080: val_acc improved from 0.82151 to 0.84486, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 0.7433 - acc: 0.8033 - val_loss: 0.7148 - val_acc: 0.8449\n",
      "Epoch 82/200\n",
      "31648/32030 [============================>.] - ETA: 0s - loss: 0.7357 - acc: 0.8047Epoch 00081: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.7362 - acc: 0.8045 - val_loss: 0.6746 - val_acc: 0.8311\n",
      "Epoch 83/200\n",
      "31296/32030 [============================>.] - ETA: 0s - loss: 0.7254 - acc: 0.8101Epoch 00082: val_acc improved from 0.84486 to 0.84769, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.7259 - acc: 0.8101 - val_loss: 0.7263 - val_acc: 0.8477\n",
      "Epoch 84/200\n",
      "31136/32030 [============================>.] - ETA: 0s - loss: 0.7216 - acc: 0.8108Epoch 00083: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.7223 - acc: 0.8104 - val_loss: 0.7415 - val_acc: 0.8272\n",
      "Epoch 85/200\n",
      "31456/32030 [============================>.] - ETA: 0s - loss: 0.7128 - acc: 0.8145Epoch 00084: val_acc improved from 0.84769 to 0.86220, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.7115 - acc: 0.8149 - val_loss: 0.6423 - val_acc: 0.8622\n",
      "Epoch 86/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 0.7084 - acc: 0.8164Epoch 00085: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.7072 - acc: 0.8165 - val_loss: 0.7512 - val_acc: 0.8031\n",
      "Epoch 87/200\n",
      "31456/32030 [============================>.] - ETA: 0s - loss: 0.6962 - acc: 0.8198Epoch 00086: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6971 - acc: 0.8196 - val_loss: 0.6544 - val_acc: 0.8256\n",
      "Epoch 88/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 0.6963 - acc: 0.8198Epoch 00087: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6956 - acc: 0.8197 - val_loss: 0.6659 - val_acc: 0.8222\n",
      "Epoch 89/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 0.6939 - acc: 0.8258Epoch 00088: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6939 - acc: 0.8259 - val_loss: 0.6892 - val_acc: 0.8183\n",
      "Epoch 90/200\n",
      "31072/32030 [============================>.] - ETA: 0s - loss: 0.6864 - acc: 0.8251Epoch 00089: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6871 - acc: 0.8245 - val_loss: 0.6575 - val_acc: 0.8399\n",
      "Epoch 91/200\n",
      "31392/32030 [============================>.] - ETA: 0s - loss: 0.6767 - acc: 0.8260Epoch 00090: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6759 - acc: 0.8259 - val_loss: 0.6558 - val_acc: 0.8463\n",
      "Epoch 92/200\n",
      "31264/32030 [============================>.] - ETA: 0s - loss: 0.6653 - acc: 0.8314Epoch 00091: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6648 - acc: 0.8313 - val_loss: 0.6079 - val_acc: 0.8466\n",
      "Epoch 93/200\n",
      "31488/32030 [============================>.] - ETA: 0s - loss: 0.6667 - acc: 0.8290Epoch 00092: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6674 - acc: 0.8295 - val_loss: 0.6605 - val_acc: 0.8240\n",
      "Epoch 94/200\n",
      "31680/32030 [============================>.] - ETA: 0s - loss: 0.6623 - acc: 0.8295Epoch 00093: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.6617 - acc: 0.8298 - val_loss: 0.6976 - val_acc: 0.8376\n",
      "Epoch 95/200\n",
      "31584/32030 [============================>.] - ETA: 0s - loss: 0.6494 - acc: 0.8347Epoch 00094: val_acc improved from 0.86220 to 0.86697, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 0.6507 - acc: 0.8349 - val_loss: 0.6243 - val_acc: 0.8670\n",
      "Epoch 96/200\n",
      "31296/32030 [============================>.] - ETA: 0s - loss: 0.6403 - acc: 0.8378Epoch 00095: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6398 - acc: 0.8375 - val_loss: 0.5918 - val_acc: 0.8468\n",
      "Epoch 97/200\n",
      "31520/32030 [============================>.] - ETA: 0s - loss: 0.6401 - acc: 0.8369Epoch 00096: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.6402 - acc: 0.8371 - val_loss: 0.6569 - val_acc: 0.8454\n",
      "Epoch 98/200\n",
      "31456/32030 [============================>.] - ETA: 0s - loss: 0.6240 - acc: 0.8376Epoch 00097: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6234 - acc: 0.8378 - val_loss: 0.6006 - val_acc: 0.8296\n",
      "Epoch 99/200\n",
      "31808/32030 [============================>.] - ETA: 0s - loss: 0.6405 - acc: 0.8373Epoch 00098: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6393 - acc: 0.8374 - val_loss: 0.6169 - val_acc: 0.8309\n",
      "Epoch 100/200\n",
      "31840/32030 [============================>.] - ETA: 0s - loss: 0.6309 - acc: 0.8416Epoch 00099: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6308 - acc: 0.8417 - val_loss: 0.6196 - val_acc: 0.8403\n",
      "Epoch 101/200\n",
      "31520/32030 [============================>.] - ETA: 0s - loss: 0.6305 - acc: 0.8434Epoch 00100: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6296 - acc: 0.8434 - val_loss: 0.5820 - val_acc: 0.8581\n",
      "Epoch 102/200\n",
      "31136/32030 [============================>.] - ETA: 0s - loss: 0.6193 - acc: 0.8410Epoch 00101: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6173 - acc: 0.8411 - val_loss: 0.5677 - val_acc: 0.8548\n",
      "Epoch 103/200\n",
      "31712/32030 [============================>.] - ETA: 0s - loss: 0.6182 - acc: 0.8432Epoch 00102: val_acc improved from 0.86697 to 0.87122, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 0.6200 - acc: 0.8428 - val_loss: 0.5875 - val_acc: 0.8712\n",
      "Epoch 104/200\n",
      "31136/32030 [============================>.] - ETA: 0s - loss: 0.6120 - acc: 0.8471Epoch 00103: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6129 - acc: 0.8468 - val_loss: 0.5884 - val_acc: 0.8585\n",
      "Epoch 105/200\n",
      "32000/32030 [============================>.] - ETA: 0s - loss: 0.6142 - acc: 0.8455Epoch 00104: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.6143 - acc: 0.8455 - val_loss: 0.6265 - val_acc: 0.8417\n",
      "Epoch 106/200\n",
      "31648/32030 [============================>.] - ETA: 0s - loss: 0.6085 - acc: 0.8491Epoch 00105: val_acc improved from 0.87122 to 0.87140, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.6083 - acc: 0.8493 - val_loss: 0.5736 - val_acc: 0.8714\n",
      "Epoch 107/200\n",
      "31520/32030 [============================>.] - ETA: 0s - loss: 0.5962 - acc: 0.8497Epoch 00106: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5956 - acc: 0.8501 - val_loss: 0.6015 - val_acc: 0.8465\n",
      "Epoch 108/200\n",
      "31712/32030 [============================>.] - ETA: 0s - loss: 0.5957 - acc: 0.8523Epoch 00107: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5939 - acc: 0.8525 - val_loss: 0.6044 - val_acc: 0.8319\n",
      "Epoch 109/200\n",
      "31712/32030 [============================>.] - ETA: 0s - loss: 0.5912 - acc: 0.8517Epoch 00108: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5910 - acc: 0.8519 - val_loss: 0.6280 - val_acc: 0.8555\n",
      "Epoch 110/200\n",
      "31040/32030 [============================>.] - ETA: 0s - loss: 0.5996 - acc: 0.8540Epoch 00109: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5999 - acc: 0.8539 - val_loss: 0.5873 - val_acc: 0.8406\n",
      "Epoch 111/200\n",
      "31616/32030 [============================>.] - ETA: 0s - loss: 0.5931 - acc: 0.8566Epoch 00110: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5931 - acc: 0.8568 - val_loss: 0.5776 - val_acc: 0.8588\n",
      "Epoch 112/200\n",
      "31936/32030 [============================>.] - ETA: 0s - loss: 0.5907 - acc: 0.8550Epoch 00111: val_acc improved from 0.87140 to 0.87953, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.5907 - acc: 0.8551 - val_loss: 0.5280 - val_acc: 0.8795\n",
      "Epoch 113/200\n",
      "32000/32030 [============================>.] - ETA: 0s - loss: 0.5940 - acc: 0.8572Epoch 00112: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5940 - acc: 0.8572 - val_loss: 0.5927 - val_acc: 0.8599\n",
      "Epoch 114/200\n",
      "31136/32030 [============================>.] - ETA: 0s - loss: 0.5854 - acc: 0.8605Epoch 00113: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5874 - acc: 0.8603 - val_loss: 0.5364 - val_acc: 0.8725\n",
      "Epoch 115/200\n",
      "32000/32030 [============================>.] - ETA: 0s - loss: 0.6010 - acc: 0.8565Epoch 00114: val_acc improved from 0.87953 to 0.88236, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 0.6013 - acc: 0.8565 - val_loss: 0.5780 - val_acc: 0.8824\n",
      "Epoch 116/200\n",
      "32000/32030 [============================>.] - ETA: 0s - loss: 0.5966 - acc: 0.8575Epoch 00115: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5967 - acc: 0.8574 - val_loss: 0.5613 - val_acc: 0.8679\n",
      "Epoch 117/200\n",
      "31232/32030 [============================>.] - ETA: 0s - loss: 0.5836 - acc: 0.8638Epoch 00116: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5851 - acc: 0.8634 - val_loss: 0.5693 - val_acc: 0.8774\n",
      "Epoch 118/200\n",
      "31328/32030 [============================>.] - ETA: 0s - loss: 0.5787 - acc: 0.8641Epoch 00117: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5800 - acc: 0.8638 - val_loss: 0.5896 - val_acc: 0.8604\n",
      "Epoch 119/200\n",
      "31136/32030 [============================>.] - ETA: 0s - loss: 0.5735 - acc: 0.8670Epoch 00118: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5738 - acc: 0.8669 - val_loss: 0.5755 - val_acc: 0.8417\n",
      "Epoch 120/200\n",
      "31168/32030 [============================>.] - ETA: 0s - loss: 0.5810 - acc: 0.8626Epoch 00119: val_acc improved from 0.88236 to 0.88608, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 0.5805 - acc: 0.8628 - val_loss: 0.6027 - val_acc: 0.8861\n",
      "Epoch 121/200\n",
      "31456/32030 [============================>.] - ETA: 0s - loss: 0.5664 - acc: 0.8651Epoch 00120: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5663 - acc: 0.8649 - val_loss: 0.5596 - val_acc: 0.8732\n",
      "Epoch 122/200\n",
      "31904/32030 [============================>.] - ETA: 0s - loss: 0.5651 - acc: 0.8683Epoch 00121: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5648 - acc: 0.8683 - val_loss: 0.5328 - val_acc: 0.8613\n",
      "Epoch 123/200\n",
      "31392/32030 [============================>.] - ETA: 0s - loss: 0.5655 - acc: 0.8658Epoch 00122: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5651 - acc: 0.8661 - val_loss: 0.5793 - val_acc: 0.8652\n",
      "Epoch 124/200\n",
      "31744/32030 [============================>.] - ETA: 0s - loss: 0.5635 - acc: 0.8686Epoch 00123: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.5634 - acc: 0.8687 - val_loss: 0.5484 - val_acc: 0.8638\n",
      "Epoch 125/200\n",
      "31936/32030 [============================>.] - ETA: 0s - loss: 0.5551 - acc: 0.8706Epoch 00124: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.5551 - acc: 0.8707 - val_loss: 0.5649 - val_acc: 0.8580\n",
      "Epoch 126/200\n",
      "31712/32030 [============================>.] - ETA: 0s - loss: 0.5485 - acc: 0.8723Epoch 00125: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.5479 - acc: 0.8723 - val_loss: 0.5275 - val_acc: 0.8691\n",
      "Epoch 127/200\n",
      "31680/32030 [============================>.] - ETA: 0s - loss: 0.5527 - acc: 0.8729Epoch 00126: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5517 - acc: 0.8730 - val_loss: 0.5575 - val_acc: 0.8698\n",
      "Epoch 128/200\n",
      "31264/32030 [============================>.] - ETA: 0s - loss: 0.5476 - acc: 0.8733Epoch 00127: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5468 - acc: 0.8735 - val_loss: 0.5638 - val_acc: 0.8716\n",
      "Epoch 129/200\n",
      "31520/32030 [============================>.] - ETA: 0s - loss: 0.5366 - acc: 0.8756Epoch 00128: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5374 - acc: 0.8752 - val_loss: 0.5311 - val_acc: 0.8500\n",
      "Epoch 130/200\n",
      "32000/32030 [============================>.] - ETA: 0s - loss: 0.5411 - acc: 0.8734Epoch 00129: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5415 - acc: 0.8734 - val_loss: 0.5230 - val_acc: 0.8848\n",
      "Epoch 131/200\n",
      "31392/32030 [============================>.] - ETA: 0s - loss: 0.5374 - acc: 0.8744Epoch 00130: val_acc improved from 0.88608 to 0.88820, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.5379 - acc: 0.8744 - val_loss: 0.5054 - val_acc: 0.8882\n",
      "Epoch 132/200\n",
      "31456/32030 [============================>.] - ETA: 0s - loss: 0.5326 - acc: 0.8760Epoch 00131: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5329 - acc: 0.8756 - val_loss: 0.4993 - val_acc: 0.8673\n",
      "Epoch 133/200\n",
      "31296/32030 [============================>.] - ETA: 0s - loss: 0.5294 - acc: 0.8761Epoch 00132: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5269 - acc: 0.8767 - val_loss: 0.5282 - val_acc: 0.8818\n",
      "Epoch 134/200\n",
      "31968/32030 [============================>.] - ETA: 0s - loss: 0.5234 - acc: 0.8778Epoch 00133: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5236 - acc: 0.8777 - val_loss: 0.5073 - val_acc: 0.8719\n",
      "Epoch 135/200\n",
      "32000/32030 [============================>.] - ETA: 0s - loss: 0.5247 - acc: 0.8771Epoch 00134: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5247 - acc: 0.8771 - val_loss: 0.5301 - val_acc: 0.8809\n",
      "Epoch 136/200\n",
      "31840/32030 [============================>.] - ETA: 0s - loss: 0.5257 - acc: 0.8809Epoch 00135: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.5253 - acc: 0.8808 - val_loss: 0.5220 - val_acc: 0.8878\n",
      "Epoch 137/200\n",
      "31424/32030 [============================>.] - ETA: 0s - loss: 0.5155 - acc: 0.8805Epoch 00136: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.5161 - acc: 0.8801 - val_loss: 0.5370 - val_acc: 0.8772\n",
      "Epoch 138/200\n",
      "31104/32030 [============================>.] - ETA: 0s - loss: 0.5105 - acc: 0.8821Epoch 00137: val_acc improved from 0.88820 to 0.88997, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 0.5099 - acc: 0.8821 - val_loss: 0.4736 - val_acc: 0.8900\n",
      "Epoch 139/200\n",
      "31776/32030 [============================>.] - ETA: 0s - loss: 0.5112 - acc: 0.8836Epoch 00138: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.5104 - acc: 0.8837 - val_loss: 0.5293 - val_acc: 0.8751\n",
      "Epoch 140/200\n",
      "31200/32030 [============================>.] - ETA: 0s - loss: 0.5111 - acc: 0.8807Epoch 00139: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5092 - acc: 0.8811 - val_loss: 0.4983 - val_acc: 0.8886\n",
      "Epoch 141/200\n",
      "32000/32030 [============================>.] - ETA: 0s - loss: 0.5083 - acc: 0.8831Epoch 00140: val_acc improved from 0.88997 to 0.90129, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.5082 - acc: 0.8832 - val_loss: 0.4725 - val_acc: 0.9013\n",
      "Epoch 142/200\n",
      "31328/32030 [============================>.] - ETA: 0s - loss: 0.4945 - acc: 0.8850Epoch 00141: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.4945 - acc: 0.8848 - val_loss: 0.4918 - val_acc: 0.8933\n",
      "Epoch 143/200\n",
      "31488/32030 [============================>.] - ETA: 0s - loss: 0.5068 - acc: 0.8853Epoch 00142: val_acc improved from 0.90129 to 0.91279, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.5042 - acc: 0.8854 - val_loss: 0.4724 - val_acc: 0.9128\n",
      "Epoch 144/200\n",
      "31776/32030 [============================>.] - ETA: 0s - loss: 0.4932 - acc: 0.8867Epoch 00143: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.4938 - acc: 0.8863 - val_loss: 0.4596 - val_acc: 0.8884\n",
      "Epoch 145/200\n",
      "31488/32030 [============================>.] - ETA: 0s - loss: 0.4962 - acc: 0.8859Epoch 00144: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.4960 - acc: 0.8862 - val_loss: 0.4282 - val_acc: 0.8889\n",
      "Epoch 146/200\n",
      "31872/32030 [============================>.] - ETA: 0s - loss: 0.4848 - acc: 0.8900Epoch 00145: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.4852 - acc: 0.8899 - val_loss: 0.4736 - val_acc: 0.8900\n",
      "Epoch 147/200\n",
      "31648/32030 [============================>.] - ETA: 0s - loss: 0.4875 - acc: 0.8869Epoch 00146: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.4872 - acc: 0.8869 - val_loss: 0.4497 - val_acc: 0.8889\n",
      "Epoch 148/200\n",
      "31520/32030 [============================>.] - ETA: 0s - loss: 0.4787 - acc: 0.8895Epoch 00147: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.4777 - acc: 0.8898 - val_loss: 0.4813 - val_acc: 0.8965\n",
      "Epoch 149/200\n",
      "31840/32030 [============================>.] - ETA: 0s - loss: 0.4866 - acc: 0.8883Epoch 00148: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.4875 - acc: 0.8882 - val_loss: 0.4448 - val_acc: 0.9091\n",
      "Epoch 150/200\n",
      "31904/32030 [============================>.] - ETA: 0s - loss: 0.4788 - acc: 0.8902Epoch 00149: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.4788 - acc: 0.8901 - val_loss: 0.5136 - val_acc: 0.8932\n",
      "Epoch 151/200\n",
      "31776/32030 [============================>.] - ETA: 0s - loss: 0.5071 - acc: 0.8909Epoch 00150: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5077 - acc: 0.8909 - val_loss: 0.4741 - val_acc: 0.8841\n",
      "Epoch 152/200\n",
      "31328/32030 [============================>.] - ETA: 0s - loss: 0.5125 - acc: 0.8901Epoch 00151: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.5137 - acc: 0.8897 - val_loss: 0.4897 - val_acc: 0.8951\n",
      "Epoch 153/200\n",
      "31936/32030 [============================>.] - ETA: 0s - loss: 0.5462 - acc: 0.8917Epoch 00152: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.5469 - acc: 0.8916 - val_loss: 0.5464 - val_acc: 0.8818\n",
      "Epoch 154/200\n",
      "31872/32030 [============================>.] - ETA: 0s - loss: 0.5459 - acc: 0.8931Epoch 00153: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.5460 - acc: 0.8933 - val_loss: 0.5210 - val_acc: 0.8986\n",
      "Epoch 155/200\n",
      "31776/32030 [============================>.] - ETA: 0s - loss: 0.4222 - acc: 0.9377Epoch 00154: val_acc improved from 0.91279 to 0.94056, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 0.4215 - acc: 0.9379 - val_loss: 0.4033 - val_acc: 0.9406\n",
      "Epoch 156/200\n",
      "31680/32030 [============================>.] - ETA: 0s - loss: 0.4165 - acc: 0.9368Epoch 00155: val_acc improved from 0.94056 to 0.94145, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 2s - loss: 0.4174 - acc: 0.9367 - val_loss: 0.3744 - val_acc: 0.9414\n",
      "Epoch 157/200\n",
      "31232/32030 [============================>.] - ETA: 0s - loss: 0.3706 - acc: 0.9367Epoch 00156: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3712 - acc: 0.9367 - val_loss: 0.3582 - val_acc: 0.9409\n",
      "Epoch 158/200\n",
      "31168/32030 [============================>.] - ETA: 0s - loss: 0.3709 - acc: 0.9372Epoch 00157: val_acc improved from 0.94145 to 0.96533, saving model to addition_zy.hdf5\n",
      "32030/32030 [==============================] - 1s - loss: 0.3698 - acc: 0.9376 - val_loss: 0.3523 - val_acc: 0.9653\n",
      "Epoch 159/200\n",
      "31424/32030 [============================>.] - ETA: 0s - loss: 0.3663 - acc: 0.9392Epoch 00158: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3672 - acc: 0.9389 - val_loss: 0.3530 - val_acc: 0.9496\n",
      "Epoch 160/200\n",
      "31968/32030 [============================>.] - ETA: 0s - loss: 0.3681 - acc: 0.9385Epoch 00159: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.3679 - acc: 0.9385 - val_loss: 0.3523 - val_acc: 0.9450\n",
      "Epoch 161/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 0.3679 - acc: 0.9354Epoch 00160: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3671 - acc: 0.9356 - val_loss: 0.3520 - val_acc: 0.9377\n",
      "Epoch 162/200\n",
      "31616/32030 [============================>.] - ETA: 0s - loss: 0.3660 - acc: 0.9400Epoch 00161: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3653 - acc: 0.9400 - val_loss: 0.3566 - val_acc: 0.9432\n",
      "Epoch 163/200\n",
      "31264/32030 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.9378Epoch 00162: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3637 - acc: 0.9378 - val_loss: 0.3439 - val_acc: 0.9480\n",
      "Epoch 164/200\n",
      "31520/32030 [============================>.] - ETA: 0s - loss: 0.3639 - acc: 0.9387Epoch 00163: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.3628 - acc: 0.9387 - val_loss: 0.3507 - val_acc: 0.9326\n",
      "Epoch 165/200\n",
      "31392/32030 [============================>.] - ETA: 0s - loss: 0.3627 - acc: 0.9374Epoch 00164: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.3633 - acc: 0.9374 - val_loss: 0.3456 - val_acc: 0.9501\n",
      "Epoch 166/200\n",
      "31680/32030 [============================>.] - ETA: 0s - loss: 0.3601 - acc: 0.9418Epoch 00165: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.3608 - acc: 0.9415 - val_loss: 0.3525 - val_acc: 0.9299\n",
      "Epoch 167/200\n",
      "31136/32030 [============================>.] - ETA: 0s - loss: 0.3602 - acc: 0.9372Epoch 00166: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3607 - acc: 0.9370 - val_loss: 0.3480 - val_acc: 0.9400\n",
      "Epoch 168/200\n",
      "31424/32030 [============================>.] - ETA: 0s - loss: 0.3548 - acc: 0.9391Epoch 00167: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3585 - acc: 0.9389 - val_loss: 0.3473 - val_acc: 0.9402\n",
      "Epoch 169/200\n",
      "31328/32030 [============================>.] - ETA: 0s - loss: 0.3549 - acc: 0.9409Epoch 00168: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3568 - acc: 0.9406 - val_loss: 0.3427 - val_acc: 0.9388\n",
      "Epoch 170/200\n",
      "31680/32030 [============================>.] - ETA: 0s - loss: 0.3510 - acc: 0.9455Epoch 00169: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3509 - acc: 0.9453 - val_loss: 0.3386 - val_acc: 0.9404\n",
      "Epoch 171/200\n",
      "31936/32030 [============================>.] - ETA: 0s - loss: 0.3508 - acc: 0.9443Epoch 00170: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3503 - acc: 0.9444 - val_loss: 0.3345 - val_acc: 0.9595\n",
      "Epoch 172/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 0.3479 - acc: 0.9445Epoch 00171: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3490 - acc: 0.9444 - val_loss: 0.3417 - val_acc: 0.9471\n",
      "Epoch 173/200\n",
      "31904/32030 [============================>.] - ETA: 0s - loss: 0.3502 - acc: 0.9447Epoch 00172: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3497 - acc: 0.9448 - val_loss: 0.3365 - val_acc: 0.9404\n",
      "Epoch 174/200\n",
      "31072/32030 [============================>.] - ETA: 0s - loss: 0.3497 - acc: 0.9435Epoch 00173: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3488 - acc: 0.9433 - val_loss: 0.3354 - val_acc: 0.9383\n",
      "Epoch 175/200\n",
      "31616/32030 [============================>.] - ETA: 0s - loss: 0.3457 - acc: 0.9444Epoch 00174: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3474 - acc: 0.9444 - val_loss: 0.3343 - val_acc: 0.9393\n",
      "Epoch 176/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 0.3471 - acc: 0.9444Epoch 00175: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3467 - acc: 0.9448 - val_loss: 0.3317 - val_acc: 0.9623\n",
      "Epoch 177/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 0.3439 - acc: 0.9447Epoch 00176: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3459 - acc: 0.9445 - val_loss: 0.3354 - val_acc: 0.9464\n",
      "Epoch 178/200\n",
      "31584/32030 [============================>.] - ETA: 0s - loss: 0.3458 - acc: 0.9446Epoch 00177: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3445 - acc: 0.9447 - val_loss: 0.3303 - val_acc: 0.9443\n",
      "Epoch 179/200\n",
      "31488/32030 [============================>.] - ETA: 0s - loss: 0.3427 - acc: 0.9450Epoch 00178: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3440 - acc: 0.9450 - val_loss: 0.3339 - val_acc: 0.9494\n",
      "Epoch 180/200\n",
      "31488/32030 [============================>.] - ETA: 0s - loss: 0.3439 - acc: 0.9432Epoch 00179: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3437 - acc: 0.9433 - val_loss: 0.3289 - val_acc: 0.9512\n",
      "Epoch 181/200\n",
      "31360/32030 [============================>.] - ETA: 0s - loss: 0.3428 - acc: 0.9454Epoch 00180: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3430 - acc: 0.9451 - val_loss: 0.3285 - val_acc: 0.9598\n",
      "Epoch 182/200\n",
      "31584/32030 [============================>.] - ETA: 0s - loss: 0.3399 - acc: 0.9459Epoch 00181: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3420 - acc: 0.9456 - val_loss: 0.3250 - val_acc: 0.9545\n",
      "Epoch 183/200\n",
      "31104/32030 [============================>.] - ETA: 0s - loss: 0.3432 - acc: 0.9448Epoch 00182: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3416 - acc: 0.9448 - val_loss: 0.3245 - val_acc: 0.9503\n",
      "Epoch 184/200\n",
      "31424/32030 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.9446Epoch 00183: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3404 - acc: 0.9446 - val_loss: 0.3268 - val_acc: 0.9533\n",
      "Epoch 185/200\n",
      "31808/32030 [============================>.] - ETA: 0s - loss: 0.3410 - acc: 0.9462Epoch 00184: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3406 - acc: 0.9463 - val_loss: 0.3238 - val_acc: 0.9505\n",
      "Epoch 186/200\n",
      "31872/32030 [============================>.] - ETA: 0s - loss: 0.3368 - acc: 0.9469Epoch 00185: val_acc did not improve\n",
      "32030/32030 [==============================] - 2s - loss: 0.3376 - acc: 0.9468 - val_loss: 0.3268 - val_acc: 0.9506\n",
      "Epoch 187/200\n",
      "31616/32030 [============================>.] - ETA: 0s - loss: 0.3404 - acc: 0.9438Epoch 00186: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3390 - acc: 0.9442 - val_loss: 0.3264 - val_acc: 0.9478\n",
      "Epoch 188/200\n",
      "31392/32030 [============================>.] - ETA: 0s - loss: 0.3371 - acc: 0.9470Epoch 00187: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3371 - acc: 0.9470 - val_loss: 0.3280 - val_acc: 0.9485\n",
      "Epoch 189/200\n",
      "31584/32030 [============================>.] - ETA: 0s - loss: 0.3355 - acc: 0.9454Epoch 00188: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3378 - acc: 0.9451 - val_loss: 0.3195 - val_acc: 0.9512\n",
      "Epoch 190/200\n",
      "31968/32030 [============================>.] - ETA: 0s - loss: 0.3364 - acc: 0.9472Epoch 00189: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3361 - acc: 0.9472 - val_loss: 0.3236 - val_acc: 0.9549\n",
      "Epoch 191/200\n",
      "31904/32030 [============================>.] - ETA: 0s - loss: 0.3360 - acc: 0.9458Epoch 00190: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3363 - acc: 0.9458 - val_loss: 0.3220 - val_acc: 0.9485\n",
      "Epoch 192/200\n",
      "31264/32030 [============================>.] - ETA: 0s - loss: 0.3342 - acc: 0.9454Epoch 00191: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3349 - acc: 0.9453 - val_loss: 0.3279 - val_acc: 0.9395\n",
      "Epoch 193/200\n",
      "31392/32030 [============================>.] - ETA: 0s - loss: 0.3348 - acc: 0.9448Epoch 00192: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3351 - acc: 0.9447 - val_loss: 0.3218 - val_acc: 0.9342\n",
      "Epoch 194/200\n",
      "31520/32030 [============================>.] - ETA: 0s - loss: 0.3331 - acc: 0.9466Epoch 00193: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3328 - acc: 0.9466 - val_loss: 0.3159 - val_acc: 0.9524\n",
      "Epoch 195/200\n",
      "31104/32030 [============================>.] - ETA: 0s - loss: 0.3356 - acc: 0.9461Epoch 00194: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3347 - acc: 0.9461 - val_loss: 0.3211 - val_acc: 0.9584\n",
      "Epoch 196/200\n",
      "31488/32030 [============================>.] - ETA: 0s - loss: 0.3324 - acc: 0.9478Epoch 00195: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3333 - acc: 0.9478 - val_loss: 0.3229 - val_acc: 0.9485\n",
      "Epoch 197/200\n",
      "31552/32030 [============================>.] - ETA: 0s - loss: 0.3321 - acc: 0.9463Epoch 00196: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3323 - acc: 0.9465 - val_loss: 0.3255 - val_acc: 0.9551\n",
      "Epoch 198/200\n",
      "31104/32030 [============================>.] - ETA: 0s - loss: 0.3322 - acc: 0.9455Epoch 00197: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3326 - acc: 0.9452 - val_loss: 0.3202 - val_acc: 0.9437\n",
      "Epoch 199/200\n",
      "31104/32030 [============================>.] - ETA: 0s - loss: 0.3317 - acc: 0.9461Epoch 00198: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3302 - acc: 0.9463 - val_loss: 0.3213 - val_acc: 0.9558\n",
      "Epoch 200/200\n",
      "31424/32030 [============================>.] - ETA: 0s - loss: 0.3307 - acc: 0.9478Epoch 00199: val_acc did not improve\n",
      "32030/32030 [==============================] - 1s - loss: 0.3291 - acc: 0.9478 - val_loss: 0.3197 - val_acc: 0.9460\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "#learning_rate = 0.001\n",
    "#decay = 0.005\n",
    "training_epochs = 200\n",
    "#batch_size = 128\n",
    "\n",
    "#K.set_value(model.optimizer.lr, learning_rate)\n",
    "#K.set_value(model.optimizer.momentum, 0.3)\n",
    "from keras.callbacks import LearningRateScheduler, ReduceLROnPlateau, ModelCheckpoint\n",
    "# learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 1.0\n",
    "    drop = 0.6\n",
    "    epochs_drop = 15.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    print(lrate)\n",
    "    return lrate\n",
    "#lrate = LearningRateScheduler(step_decay)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2,\n",
    "              patience=10, min_lr=0.0003)\n",
    "model_chkpt = ModelCheckpoint(filepath='addition_zy.hdf5', monitor='val_acc', verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=training_epochs, \n",
    "                    shuffle=True, \n",
    "                    validation_split=0.15,\n",
    "                   callbacks=[reduce_lr, model_chkpt])\n",
    "# callbacks=[lrate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGSCAYAAADAX5pxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\np8AiIiIiXu//A5os7DSvZKyKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9441f2390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_train(hist):\n",
    "    h = hist.history\n",
    "    if 'acc' in h:\n",
    "        meas='acc'\n",
    "        loc='lower right'\n",
    "    else:\n",
    "        meas='loss'\n",
    "        loc='upper right'\n",
    "    plt.style.use('ggplot')\n",
    "    plt.plot(hist.history[meas])\n",
    "    plt.plot(hist.history['val_'+meas])\n",
    "    plt.title('model '+meas)\n",
    "    plt.ylabel(meas)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc=loc)\n",
    "    \n",
    "plot_train(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"addition_zy.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.392450018311\n",
      "Test accuracy: 0.9534\n"
     ]
    }
   ],
   "source": [
    "# Now try testing on unseen data\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105 104 111 106  99]\n",
      "[105 104 111 106  99]\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "print(np.argmax(pred[:5], axis=1))\n",
    "print(np.argmax(y_test[:5], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad, but can we confirm that it has generalised to examples it hasn't seen?\n",
    "\n",
    "Try testing on the removed numbers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "277 samples generated\n",
      "Test loss: 0.290400463429\n",
      "Test accuracy: 0.967509030005\n",
      "Hidden numbers: [23 71 99]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>y</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99</td>\n",
       "      <td>40</td>\n",
       "      <td>139</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99</td>\n",
       "      <td>44</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99</td>\n",
       "      <td>45</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71</td>\n",
       "      <td>31</td>\n",
       "      <td>102</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>71</td>\n",
       "      <td>79</td>\n",
       "      <td>150</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>71</td>\n",
       "      <td>48</td>\n",
       "      <td>119</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>71</td>\n",
       "      <td>73</td>\n",
       "      <td>144</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x0  x1    y  y_pred\n",
       "0  23  20   43      43\n",
       "1  99  40  139     139\n",
       "2  71   3   74      74\n",
       "3  99  44  143     143\n",
       "4  99  45  144     144\n",
       "5  71  31  102     102\n",
       "6  71  79  150     150\n",
       "7  71  48  119     119\n",
       "8  23   2   25      25\n",
       "9  71  73  144     144"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gen_special_numbers(samples, num_to_hide):\n",
    "    X = pd.DataFrame(np.random.randint(input_range_lo, input_range_hi+1, size=2*samples,).reshape(samples, 2))\n",
    "    X['y'] = X[0] + X[1]\n",
    "    X = X[(X[0].isin(num_to_hide)) | (X[0].isin(num_to_hide))]\n",
    "    print(len(X), \"samples generated\")\n",
    "    y = X['y'].values\n",
    "    X = X[[0,1]].values\n",
    "    return X, y\n",
    "\n",
    "X_test_new, y_test_new = gen_special_numbers(10000, num_to_hide)\n",
    "X_test_new_float = transform_X(X_test_new)\n",
    "y_test_new = keras.utils.to_categorical(y_test_new, num_classes=n_classes)\n",
    "score = model.evaluate(X_test_new_float, y_test_new, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "pred = model.predict(X_test_new_float)\n",
    "sample_final_results = pd.DataFrame({'x0':X_test_new[:,0],\n",
    "                                    'x1':X_test_new[:,1],\n",
    "                                    'y':np.argmax(y_test_new, axis=1),\n",
    "                                    'y_pred': np.argmax(pred, axis=1)}).sample(n=10).reset_index(drop=True)\n",
    "print(\"Hidden numbers:\", num_to_hide)\n",
    "sample_final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: FIND MINIMAL MODEL SIZE / SAMPLE SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding remarks\n",
    "Our network has managed to generalise to unseen inputs. There are still sources of error, most likely coming from examples with fewer classes (e.g. the ones near 0 and 200). Another possibility of data generation is to iterate through output samples and fix the number for each class."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python35]",
   "language": "python",
   "name": "conda-env-python35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
